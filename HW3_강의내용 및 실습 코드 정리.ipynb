{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff356f91",
   "metadata": {},
   "source": [
    "# Text mining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63559cd",
   "metadata": {},
   "source": [
    "## What is the 'Text mining'?\n",
    "> #### the process of *deriving high-quality information from text.*\n",
    "> #### Unstructured text -> structured data로 변환!\n",
    "> #### 일정한 길이의 *vector*로 변환\n",
    "> #### 이렇게 변화시킨 vector에 머신러닝(딥러닝) 기법을 적용!\n",
    "-----\n",
    "### text mining의 적용 분야\n",
    "> 1. Document classification - sentiment analysis, classification\n",
    "> 2. Document generation - Q&A, summarization, translation\n",
    "> 3. Keyword extraction - tagging/annotation \n",
    "> 4. Topic modeling - LSA(Latent Semantic Analysis), LDA(Latent Dirichelt Allocation)\n",
    "\n",
    "-----\n",
    "### text mining 도구 - Python \n",
    "> 1. NLTK - 가장 대중적인 NLP 라이브러리\n",
    "> 2. Scikit Learn - 머신러닝 라이브러리\n",
    "> 3. Genism - 텍스트 관련 도구 지원\n",
    "> 4. Keras - 딥러닝 위주의 라이브러리 제공 \n",
    "> 5. Pytorch - 최근 가장 많이 사용되는 패키지\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7441710",
   "metadata": {},
   "source": [
    "## Text mining 방법\n",
    "### NLP(Natural Language Processing) 기본도구 \n",
    "> 목적: document, sentence 등의 word sequence를 sparse, vector로 변환\n",
    "\n",
    "#### Tokenize\n",
    "> Document를 Sentence의 집합으로 분리 / Sentence를 Word 집합으로 분리 + 의미없는 문자 등을 걸러냄\n",
    "\n",
    "#### Text Normalization\n",
    "> 동일한 의미의 단어가 다른 형태를 갖는 것을 보완\n",
    "> 1. stemming(어간 추출) - 의미가 아닌 규칙에 의한 변환 (ex. 영어의 -ing는 진행형)\n",
    "> 2. Lemmatization(표제어 추출) - 사전을 이용하여 단어의 원형을 추출 \n",
    "\n",
    "#### Pos-tagging \n",
    "> 토큰화와 정규화 작업을 통해 나누어진 형태소에 대해 *품사*를 결정하여 할당하는 작업\n",
    "\n",
    "#### Chunking \n",
    "> 언어학적으로 *말모음*을 뜻함, 주어와 동사가 없는 두 단어 이상의 집합인 구(phrase)를 의미함\n",
    "\n",
    "#### BOW (Bag of Words)\n",
    "> 문서를 bag of words로 표현 -> 단어가 쓰여진 순서는 무시하며, 단어의 유/무 대신 단어가 문서에 *나타난 횟수*로 표현\n",
    ">> *Similiarity Matching* or *Classify*로 활용할 수 있음\n",
    "\n",
    "#### TFIDF(Term Frequency - Inverse Document Frequency)\n",
    "> 단어의 count를 단어가 나타난 문서의 수로 나눠서 자주 등장하지 않는 단어의 wieght을 올림 \n",
    ">> 1. tf(d,t): 문서 d에 단어 t가 나타난 횟수\n",
    ">> 2. df(t): 전체 문서 중에서 단어 t를 포함하는 문서의 수\n",
    ">> 3. idf(t): dt(t)의 역수\n",
    "\n",
    "#### BOW/TFIDF를 활용하여 Text Classification \n",
    "##### 1. Naïve Bayes : text categorization에 가장 자주 쓰이는 method \n",
    "##### 2. Logistic regression (Ridge regression, Lasso regression) : 분류를 위한 회귀분석 \n",
    "##### 3. Decision tree (Random Forset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b1577",
   "metadata": {},
   "source": [
    "## Text mining의 문제점\n",
    "\n",
    "### 1. Curse of Dimensionality (차원의 저주)\n",
    "> 각 데이터 간의 거리가 너무 멀게 위치\n",
    ">> 해결방법 : 더 많은 데이터를 사용!, 차원을 축소(dimension reduction)\n",
    "\n",
    "### 2. 단어 빈도의 불균형\n",
    "> Zipf's law (멱볍칙) - 소수의 데이터가 결정적인 결과에 영향을 미침\n",
    ">> 해결방법 : 빈도 높은 단어 삭제, Bollean Bow 사용\n",
    "\n",
    "### 3. 단어가 쓰인 순서정보의 손실\n",
    "> Loss of sequence information \n",
    ">> 해결방법 : n-gram, Deep learning\n",
    "\n",
    "## 문제 해결 방법\n",
    "###  Dimensionality Reduction - 차원의 축소\n",
    "> 1. Freature selection - Manual, Regularization\n",
    "> 2. Feature extraction - PCA(주성분 분석), LSA(SVD:특이값 분해)\n",
    "> 3. Embedding - Word embedding, Document embedding\n",
    "> 4. Deep learning - RBM(Restricted Boltzmann Machine), Autoencoder(RBM과 유사한 개념)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a0d6b",
   "metadata": {},
   "source": [
    "# 한국어 처리 koNLPy 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26db913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kolaw\n",
    "c = kolaw.open('constitution.txt').read() #대한민국헌법 텍스트 파일 로드!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25a59c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "18884\n",
      "대한민국헌법\n",
      "\n",
      "유구한 역사와 전통에 빛나는 우리 대한국민은 3·1운동으로 건립된 대한민국임시정부의 법통과 불의에 항거한 4·19민주이념을 계승하고, 조국의 민주개혁과 평화적 통일의 사명에 입각하여 정의·인도와 동포애로써 민족의 단결을 공고히 하고, 모든 사회적 폐습과 불의를 타파하며, 자율과 조화를 바탕으로 자유민주적 기본질서를 더욱 확고히 하여 정치·경제·사회·문화의 모든 영역에 있어서 각인의 기회를 균등히 하고, 능력을 최고도로 발휘하게 하며, 자유와 권리에 따르는 책임과 의무를 완수하게 하여, 안으로는 국민생활의 균등한 향상을 기하고 밖으로는 항구적인 세계평화와 인류공영에 이바지함으로써 우리들과 우리들의 자손의 안전과 자유와 행복을 영원히 확보할 것을 다짐하면서 1948년 7월 12일에 제정되고 8차에 걸쳐 개정된 헌법을 이제 국회의 의결을 거쳐 국민투표에 의하여 개정한다.\n",
      "\n",
      "       제1장 총강\n",
      "  제1조 ① 대한민국은 민주공화국이다.\n",
      "②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.\n",
      "  제2조 ① 대한민국의 국민이 되는 요건은 법률로 정한다.\n",
      "②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.\n",
      "  제3조 대한민\n"
     ]
    }
   ],
   "source": [
    "print(type(c)) #C에 저장한 텍스트 파일의 type을 확인\n",
    "print(len(c))\n",
    "print(c[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47cdb64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357\n",
      "['대한민국헌법\\n\\n유구한 역사와 전통에 빛나는 우리 대한국민은 3·1운동으로 건립된 대한민국임시정부의 법통과 불의에 항거한 4·19민주이념을 계승하고, 조국의 민주개혁과 평화적 통일의 사명에 입각하여 정의·인도와 동포애로써 민족의 단결을 공고히 하고, 모든 사회적 폐습과 불의를 타파하며, 자율과 조화를 바탕으로 자유민주적 기본질서를 더욱 확고히 하여 정치·경제·사회·문화의 모든 영역에 있어서 각인의 기회를 균등히 하고, 능력을 최고도로 발휘하게 하며, 자유와 권리에 따르는 책임과 의무를 완수하게 하여, 안으로는 국민생활의 균등한 향상을 기하고 밖으로는 항구적인 세계평화와 인류공영에 이바지함으로써 우리들과 우리들의 자손의 안전과 자유와 행복을 영원히 확보할 것을 다짐하면서 1948년 7월 12일에 제정되고 8차에 걸쳐 개정된 헌법을 이제 국회의 의결을 거쳐 국민투표에 의하여 개정한다.', '제1장 총강\\n  제1조 ① 대한민국은 민주공화국이다.', '②대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.', '제2조 ① 대한민국의 국민이 되는 요건은 법률로 정한다.', '②국가는 법률이 정하는 바에 의하여 재외국민을 보호할 의무를 진다.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\space\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize #nltk를 이용하여 tokenize -> sent로 tokenize\n",
    "c_sent = sent_tokenize(c)\n",
    "print(len(c_sent))\n",
    "print(c_sent[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a6069",
   "metadata": {},
   "source": [
    "### 한글은 nltk로 tokenize가 월활히 되지 않음!!\n",
    "### 따라서, konlpy 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf0a183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4640"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word로 tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "c_words = word_tokenize(c) #word로 tokenize함\n",
    "len(c_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b9013f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "java.lang.UnsatisfiedLinkError: Native Library C:\\Users\\space\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\_jpype.cp38-win_amd64.pyd already loaded in another classloader",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp/ipykernel_13736/268007301.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOkt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mokt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOkt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtokens_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mokt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#형태소 단위로 tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\space\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\konlpy\\tag\\_okt.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, jvmpath, max_heap_size)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjvmpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_heap_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misJVMStarted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mjvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_jvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjvmpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_heap_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0moktJavaPackage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJPackage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'kr.lucypark.okt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\space\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\konlpy\\jvm.py\u001b[0m in \u001b[0;36minit_jvm\u001b[1;34m(jvmpath, max_heap_size)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mjvmpath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         jpype.startJVM(jvmpath, '-Djava.class.path=%s' % classpath,\n\u001b[0m\u001b[0;32m     65\u001b[0m                                 \u001b[1;34m'-Dfile.encoding=UTF8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                 \u001b[1;34m'-ea'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-Xmx{}m'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_heap_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\space\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\jpype\\_core.py\u001b[0m in \u001b[0;36mstartJVM\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         _jpype.startup(jvmpath, tuple(args),\n\u001b[0m\u001b[0;32m    227\u001b[0m                        ignoreUnrecognized, convertStrings, interrupt)\n\u001b[0;32m    228\u001b[0m         \u001b[0minitializeResources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: java.lang.UnsatisfiedLinkError: Native Library C:\\Users\\space\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\_jpype.cp38-win_amd64.pyd already loaded in another classloader"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "tokens_c = okt.morphs(c) #형태소 단위로 tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7830f8ea",
   "metadata": {},
   "source": [
    "## koNLPy 설치 실패로 지속적 오류,,, \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
